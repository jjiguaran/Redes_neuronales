{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"soOxmZvkgmD5"},"source":["# Directorio donde se encuentra el dataset\n","root_path = ''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dDOS0BnHVyZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606258319554,"user_tz":180,"elapsed":6133,"user":{"displayName":"Redes Neuronales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gige9Oo1hxhNPn72H65kzsRa5nIZBRHoGsUbiva=s64","userId":"11347185938143135425"}},"outputId":"2d1d8e87-f16e-46cb-ed23-89c4adc53414"},"source":["# Imports\n","import csv\n","import random\n","import numpy as np\n","import re\n","from sklearn.feature_extraction.text import CountVectorizer\n","from gensim.models import Word2Vec, Phrases\n","from keras.preprocessing.sequence import pad_sequences\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"yKz-v7mwV2yK","executionInfo":{"status":"ok","timestamp":1606258713746,"user_tz":180,"elapsed":3503,"user":{"displayName":"Redes Neuronales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gige9Oo1hxhNPn72H65kzsRa5nIZBRHoGsUbiva=s64","userId":"11347185938143135425"}}},"source":["# Lectura del dataset de train\n","trainfile = csv.reader(open(root_path + \"train.csv\"), delimiter='\\t')\n","trainrows = np.array([[c for c in row] for row in trainfile])\n","row_count_train, column_count = np.shape(trainrows)\n","T_train = [int(c) for c in trainrows[:, 0]]\n","P_train = trainrows[:, 1]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"sdBggoUEZgP-","executionInfo":{"status":"ok","timestamp":1606258723839,"user_tz":180,"elapsed":3873,"user":{"displayName":"Redes Neuronales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gige9Oo1hxhNPn72H65kzsRa5nIZBRHoGsUbiva=s64","userId":"11347185938143135425"}}},"source":["# Preprocesamiento de los textos\n","stopwords = nltk.corpus.stopwords.words('english')\n","lemmatizer = nltk.stem.WordNetLemmatizer()       \n","\n","# Eliminación de stop-wrods y stemming de los términos\n","P_train = [re.sub(\"[^a-zA-Z]\", \" \", l.lower()) for l in P_train]\n","P_train = [l.split() for l in P_train] \n","P_train = [[lemmatizer.lemmatize(l) for l in row if l not in stopwords] for row in P_train]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FO3uMg4ke1Pj","executionInfo":{"status":"ok","timestamp":1606258817943,"user_tz":180,"elapsed":1769,"user":{"displayName":"Redes Neuronales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gige9Oo1hxhNPn72H65kzsRa5nIZBRHoGsUbiva=s64","userId":"11347185938143135425"}},"outputId":"3095a1b9-cf0b-4b28-ec91-4faa6a93405f"},"source":["# Ensayo 1\n","# Creación del modelo de word2vec\n","\n","embedding_size = 256\n","model = Word2Vec(sentences = P_train, size=embedding_size, min_count=3, window=5)\n","\n","# Convertir features word2vec\n","\n","vocab = model.wv.vocab\n","keys = list(vocab.keys())\n","filter_unknown = lambda word: vocab.get(word, None) is not None\n","encode = lambda review: list(map(keys.index, filter(filter_unknown, review)))\n","word_vector = list(map(encode, P_train))\n","\n","input_length = 150\n","X = pad_sequences(sequences=word_vector, maxlen=input_length, padding='post')\n","Y = np.array(T_train)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2758,)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"-mCIUNu4vGp7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606250500484,"user_tz":180,"elapsed":806,"user":{"displayName":"Redes Neuronales","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gige9Oo1hxhNPn72H65kzsRa5nIZBRHoGsUbiva=s64","userId":"11347185938143135425"}},"outputId":"30bffad1-abbc-400b-b1d4-e862723576e9"},"source":["# Ensayo 2 \n","# Creación del modelo de bag of words\n","\n","input_length = 1000\n","cv = CountVectorizer(max_features= input_length)\n","P_train_2 = [\"\".join(row) for row in P_train]\n","P_feat_train = cv.fit_transform(P_train_2).toarray()\n","\n","cv._validate_vocabulary()\n","\n","P_feat_train = [\"\".join(row) for row in P_train]\n","X = cv.transform(P_feat_train).toarray()\n","Y = np.array(T_train)\n","Y.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2758,)"]},"metadata":{"tags":[]},"execution_count":19}]}]}